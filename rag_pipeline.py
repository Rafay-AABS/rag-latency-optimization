import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langfuse.langchain import CallbackHandler

load_dotenv()

# Disable Langfuse if keys are invalid/placeholders
# if os.getenv("LANGFUSE_PUBLIC_KEY", "").startswith("pk-lf-ef6c6773"):
#     os.environ["LANGFUSE_SECRET_KEY"] = ""
#     os.environ["LANGFUSE_PUBLIC_KEY"] = ""
#     os.environ["LANGFUSE_HOST"] = ""

class SpeculativeRAG:
    def __init__(self):
        self.vectorstore = None
        self.retriever = None
        self.chain = None
        # Initialize Langfuse Handler
        self.langfuse_handler = CallbackHandler()
        
        # Initialize Models
        # 1. Embeddings: Local (HuggingFace) to avoid API limits
        self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # 2. Drafter LLM: Groq (Llama 3 8B - Fast)
        self.drafter_llm = ChatGroq(
            model="llama-3.1-8b-instant",
            temperature=0
        )
        
        # 3. Verifier LLM: Groq (Llama 3 70B - More capable for verification)
        # Switched from Gemini to avoid Rate Limits
        self.verifier_llm = ChatGroq(
            model="llama-3.3-70b-versatile",
            temperature=0
        )

    def ingest_pdf(self, pdf_path):
        print(f"Loading PDF: {pdf_path}")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        
        print("Splitting documents...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        
        print("Creating vector store (this may take a moment)...")
        self.vectorstore = Chroma.from_documents(
            documents=splits, 
            embedding=self.embeddings,
            collection_name="rag_collection"
        )
        self.retriever = self.vectorstore.as_retriever()
        print("Ingestion complete.")
        
        self._build_chain()

    def _build_chain(self):
        # 1. Draft Step (Groq)
        draft_template = """You are a helpful assistant. Answer the question based ONLY on the following context.
        If you don't know the answer, say that you don't know.
        
        Context:
        {context}
        
        Question: {question}
        
        Draft Answer:"""
        
        draft_prompt = ChatPromptTemplate.from_template(draft_template)
        
        # 2. Verify/Refine Step (Gemini)
        verify_template = """You are a senior editor and fact-checker. 
        You have been given a Question, some Context, and a Draft Answer generated by a junior assistant.
        
        Your task is to:
        1. Check if the Draft Answer is supported by the Context.
        2. If it is correct and well-supported, output the Draft Answer as is.
        3. If it is missing information or incorrect based on the Context, rewrite it to be correct and comprehensive.
        
        Context:
        {context}
        
        Question: {question}
        
        Draft Answer:
        {draft_answer}
        
        Final Refined Answer:"""
        
        verify_prompt = ChatPromptTemplate.from_template(verify_template)

        # Chain Construction
        
        # Step 1: Retrieve and Draft
        draft_chain = (
            RunnablePassthrough.assign(context=(lambda x: x["context"]))
            | draft_prompt
            | self.drafter_llm
            | StrOutputParser()
        )
        
        # Step 2: Verify
        # We need to pass context, question, and draft_answer to the verifier
        final_chain = (
            RunnableParallel(
                context=self.retriever,
                question=RunnablePassthrough()
            )
            .assign(draft_answer=draft_chain)
            .assign(final_answer=verify_prompt | self.verifier_llm | StrOutputParser())
        )
        
        self.chain = final_chain

    def ask(self, question):
        if not self.chain:
            return "Please ingest a PDF first."
            
        print(f"Processing question: {question}")
        # Run the chain with Langfuse callback
        config = {}
        if self.langfuse_handler:
            config["callbacks"] = [self.langfuse_handler]
            
        result = self.chain.invoke(question, config=config)
        
        return {
            "draft": result["draft_answer"],
            "final": result["final_answer"]
        }
